{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding datasets that have stopped being updated\n",
    "This notebook shows how to find datasets that have stopped being updated.\n",
    "It looks across all datasets and finds the datasets that have been stopped and then \n",
    "does analysis with the table types to show which have the highest percentage of being stopped.\n",
    "This can be extended to find for departments that have stopped releasing certain types of data, what other types are they still releasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpolicedata as opd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "opd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the parameters for the analysis that can be adjusted\n",
    "\n",
    "# Flag to remove California data to make sure it is not driving the findings\n",
    "remove_california_stops_data = True\n",
    "\n",
    "# Flag to create a debug file with removed datasets\n",
    "create_debug_file = False\n",
    "\n",
    "# The below parameters are used to configure the final graphs\n",
    "\n",
    "# These are the minimum and maximum years for what the final graphs will show\n",
    "analysis_year_min = 1960\n",
    "analysis_year_max = 2024\n",
    "\n",
    "# This is the minimum number of counts needed to show a table type in the final graphs\n",
    "minimum_tabletype_counts_to_show = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions go in this cell\n",
    "def remove_rows(remove_condition, all_datasets, debug_filename=None):\n",
    "    \n",
    "    starting_all_datasets_count = all_datasets.shape[0]\n",
    "    \n",
    "    remove_datasets = all_datasets[remove_condition]\n",
    "        \n",
    "    # # Drop the identified rows\n",
    "    all_datasets.drop(remove_datasets.index, inplace=True)\n",
    "        \n",
    "    # Verify the number of datasets removed\n",
    "    assert len(remove_datasets) == (starting_all_datasets_count - all_datasets.shape[0]), \\\n",
    "        \"Mismatch in the number of datasets removed\"\n",
    "        \n",
    "    # Save removed datasets if a debug filename is provided\n",
    "    if debug_filename:\n",
    "        remove_datasets.to_csv(debug_filename, index=True)\n",
    "    \n",
    "    return all_datasets\n",
    "\n",
    "# Used to create debug files if the create_debug_file flag is set to True\n",
    "get_fn = lambda fn: fn if create_debug_file else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell filters out datasets that are not of interest to the analysis\n",
    "\n",
    "# For the analysis we will exclude the openpolicing.stanford.edu and muckrock to prevent duplicates\n",
    "all_datasets_original = opd.datasets.query()\n",
    "# all_datasets is the filtered dataset with only the rows we are interested in\n",
    "all_datasets = all_datasets_original.copy()\n",
    "\n",
    "# Remove datasets where the data is not supplied by the agency itself\n",
    "# supplying_entity is only filled out in this case. Ignore one where OPD is hosting the data\n",
    "# OPD only hosts datasets that were released by the agency but the agency stopped.\n",
    "# Remove datasets where the data is not supplied by the agency itself\n",
    "all_datasets=remove_rows(all_datasets['supplying_entity'].apply(lambda x: not (pd.isnull(x) or 'OpenPoliceData' in x)),\n",
    "                                     all_datasets,\n",
    "                                     debug_filename=get_fn(\"removed_bad_supplying_entity_datasets.csv\"))\n",
    "\n",
    "# Remove all datasets whose State column is equal to California and are of type STOPS\n",
    "\n",
    "if remove_california_stops_data:\n",
    "    all_datasets=remove_rows(((all_datasets['State'].str.contains('California')) & (all_datasets['TableType'].str.contains('STOPS'))),\n",
    "                                     all_datasets,\n",
    "                                     debug_filename=get_fn(\"removed_california_stops_datasets.csv\"))\n",
    "    \n",
    "\n",
    "# Remove datasets where dates don't apply\n",
    "# all_datasets = all_datasets[all_datasets['Year']!='NONE']\n",
    "# Remove datasets where the year is not specified\n",
    "all_datasets=remove_rows(all_datasets['Year'].apply(lambda x: x == 'NONE'),\n",
    "                                     all_datasets,\n",
    "                                     debug_filename=get_fn(\"removed_year_none_datasets.csv\"))\n",
    "                  \n",
    "# focus on the main type for example change\n",
    "# \"USE OF FORCE - INCIDENTS\"\n",
    "# \"USE OF FORCE - SUBJECTS/OFFICERS\"\n",
    "# to just \"USE OF FORCE\"\n",
    "all_datasets['TableType'] = all_datasets['TableType'].str.split(' - ').str[0]\n",
    "\n",
    "# Combine stops and traffic stops datasets. Also include pedestrian stops.\n",
    "all_datasets['TableType'] = all_datasets['TableType'].apply(lambda x: 'STOPS' if 'STOPS' in x else x)\n",
    "\n",
    "print(f\"Excluded a total of {all_datasets_original.shape[0] - all_datasets.shape[0]} datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will organize the data to make it easier to analyze\n",
    "# Since we are interested in types datasets that have been stopped we want to focus on coverage start and end dates and the types\n",
    "selected_columns = ['State', 'SourceName', 'TableType', 'coverage_start', 'coverage_end']\n",
    "original_df = all_datasets[selected_columns].copy()\n",
    "\n",
    "# Drop rows with NaN values in 'coverage_start' and 'coverage_end' should be YEAR NONE\n",
    "df = original_df.dropna(subset=['coverage_start', 'coverage_end'])\n",
    "record_count_without_coverage_start_end = len(original_df) - len(df)\n",
    "\n",
    "# Print the number of records dropped\n",
    "print(f\"Number of records dropped with N/A in either coverage_start or coverage_end: {record_count_without_coverage_start_end}\")\n",
    "\n",
    "# Create entries for each year instead of a range of years. This will create more data in a row, but will make it easier to filter the data\n",
    "df['ListOfYears'] = df.apply(\n",
    "    lambda row: list(range(int(row['coverage_start'].year), int(row['coverage_end'].year) + 1)), axis=1)\n",
    "\n",
    "# no need for the coverage_start and coverage_end columns since we have the list of years\n",
    "df = df.drop(columns=['coverage_start', 'coverage_end'])\n",
    "\n",
    "# find any columns of 'ListOfYears' that are not list objects and print those rows\n",
    "non_list_years = df[~df['ListOfYears'].apply(lambda x: isinstance(x, list))]\n",
    "\n",
    "if len(non_list_years) > 0:\n",
    "    print(f\"Error, the ListOfYears column has {len(non_list_years)} values that are not lists are:\")\n",
    "    print(non_list_years)\n",
    "else:\n",
    "    pass # do nothing this is just a quality check, everything should be a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on organizing up the ListOfYears column\n",
    "# Finally check for duplicate values which should be zero\n",
    "\n",
    "# merge rows that have the same values by extending the 'ListOfYears' column values into a single list\n",
    "# Specifically this will merge table types that had \" - \" such as \"USE OF FORCE - INCIDENTS\" and \"USE OF FORCE - SUBJECTS/OFFICERS\"\n",
    "df = df.groupby(['State', 'SourceName', 'TableType']).agg({'ListOfYears': 'sum'}).reset_index()\n",
    "# remove any duplicate years from the 'ListOfYears' column and sort the list\n",
    "df['ListOfYears'] = df['ListOfYears'].apply(lambda x: sorted(list(set(x))))\n",
    "\n",
    "duplicates = df[df.duplicated(['State', 'SourceName', 'TableType'], keep=False)]\n",
    "if len(duplicates) > 0:\n",
    "    print(f\"Error, the dataset has {len(duplicates)} duplicate rows. They are:\")\n",
    "    print(duplicates)\n",
    "else:\n",
    "    pass # do nothing since just another quality check\n",
    "\n",
    "# count all the unique 'State', 'SourceName', and 'TableType' combinations \n",
    "unique_combinations = df.groupby(['State', 'SourceName', 'TableType']).size().reset_index(name='Count')\n",
    "\n",
    "if len(unique_combinations) != len(df):\n",
    "    print(f\"Error, the dataset has {len(df) - len(unique_combinations)} duplicate rows. They are:\")\n",
    "    print(df[df.duplicated(['State', 'SourceName', 'TableType'], keep=False)])\n",
    "else:\n",
    "    pass # do nothing since just another quality check\n",
    "\n",
    "# verify all the # ListOfYears are lists\n",
    "non_list_years = df[~df['ListOfYears'].apply(lambda x: isinstance(x, list))]\n",
    "if len(non_list_years) > 0:\n",
    "    print(f\"Error, the ListOfYears column has {len(non_list_years)} values that are not lists are:\")\n",
    "    print(non_list_years)\n",
    "else:\n",
    "    pass # do nothing this is just a quality check, everything should be a list\n",
    "\n",
    "# Calculate the minimum and maximum years from the combined lists to simiplify the plotting\n",
    "df['MinYear'] = df['ListOfYears'].apply(min)\n",
    "df['MaxYear'] = df['ListOfYears'].apply(max)\n",
    "\n",
    "# Create a label for plotting\n",
    "df['Label'] = df['SourceName'] + ', ' + df['State'] + ', ' + df['TableType']\n",
    "\n",
    "# find any rows where the MinYear is greater than the MaxYear or the MinYear is less than 2000 or either the MinYear or MaxYear are not integers\n",
    "invalid_years = df[(df['MinYear'] > df['MaxYear']) | (df['MinYear'] < analysis_year_min) | (df['MinYear'] % 1 != 0) | (df['MaxYear'] % 1 != 0)]\n",
    "\n",
    "if len(invalid_years) > 0:\n",
    "    print(f\"Error, the dataset has {len(invalid_years)} rows with invalid years. They are:\")\n",
    "    print(invalid_years)\n",
    "else:\n",
    "    pass # do nothing this is just a quality check, everything should be a list\n",
    "\n",
    "print(\"If nothing else was printed that means the data is clean and ready for analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df= df.copy()\n",
    "\n",
    "# If the MinYear is the same as the MaxYear, then we will add a year to the MaxYear\n",
    "plot_df['MaxYear'] = plot_df.apply(lambda row: row['MaxYear'] + 1 if row['MinYear'] == row['MaxYear'] else row['MaxYear'], axis=1)\n",
    "\n",
    "# Increase min year to shrink x-limits\n",
    "min_year = 2010\n",
    "plot_df['MinYear'] = plot_df['MinYear'].apply(lambda x: min_year if x<min_year else x)\n",
    "\n",
    "plot_df = plot_df.sort_values(by=['TableType','MaxYear'])\n",
    "\n",
    "# Plot the data to see the years of data available for each unique combination of 'SourceName', 'State', 'TableType'\n",
    "plt.figure(figsize=(7, 60))\n",
    "ax = plt.barh(plot_df['Label'], plot_df['MaxYear'] - plot_df['MinYear'], left=plot_df['MinYear'], color='blue')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Source, State, Table Type')\n",
    "plt.title('Years of Data Available by Source, State, and Table Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the datasets where the data is most likely to be stopped within the year\n",
    "\n",
    "\n",
    "tabletype_analysis_df = df.copy()\n",
    "tabletype_counts = tabletype_analysis_df['TableType'].value_counts()\n",
    "\n",
    "# show only statistically significant table types\n",
    "total_tabletype_counts = len(tabletype_counts)\n",
    "tabletype_counts = tabletype_counts[tabletype_counts >= minimum_tabletype_counts_to_show]\n",
    "print(f\"Table type counts (tabletype_counts >= {minimum_tabletype_counts_to_show}) length is {len(tabletype_counts)}. The total number of table types is {total_tabletype_counts}\")\n",
    "\n",
    "# filter out the table types that are not in the tabletype_counts\n",
    "tabletype_analysis_df = tabletype_analysis_df[tabletype_analysis_df['TableType'].isin(tabletype_counts.index)]\n",
    "\n",
    "stopped_datasets = tabletype_analysis_df[tabletype_analysis_df['MaxYear'] < (analysis_year_max-1)]\n",
    "stopped_tabletype_counts = stopped_datasets['TableType'].value_counts()\n",
    "\n",
    "\n",
    "# compute a bar graph histogram of the number of datasets that are stopped by TableType\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\n",
    "\n",
    "tabletype_counts.plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_xlabel('Table Type')\n",
    "axes[0].set_ylabel('Number of Datasets')\n",
    "axes[0].set_title('Number of All Datasets by Table Type')\n",
    "axes[0].set_xticklabels(tabletype_counts.index, rotation=45, ha='right')\n",
    "\n",
    "stopped_tabletype_counts.plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_xlabel('Table Type')\n",
    "axes[1].set_ylabel('Number of Datasets')\n",
    "axes[1].set_title('Number of Stopped Datasets by Table Type')\n",
    "axes[1].set_xticklabels(stopped_tabletype_counts.index, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find which type has the highest ratio of stopped datasets\n",
    "percentage_of_stopped_datasets = 100*(stopped_tabletype_counts / tabletype_counts)\n",
    "percentage_of_stopped_datasets = percentage_of_stopped_datasets.fillna(0)\n",
    "\n",
    "# Create a bar plot of the ratio and sort the values from high to low\n",
    "percentage_of_stopped_datasets = percentage_of_stopped_datasets.sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = percentage_of_stopped_datasets.plot(kind='bar')\n",
    "ax.set_xticklabels(percentage_of_stopped_datasets.index, rotation=45, ha='right')\n",
    "plt.xlabel('Table Type')\n",
    "plt.ylabel('Percentage of Stopped Datasets to All Datasets')\n",
    "plt.title('Percentage of Stopped Datasets to All Datasets by Table Type')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
